"""
ç®¡ç†å¤„ç†å™¨ - æ”¯æŒæ™ºèƒ½æŸ¥è¯¢çš„æ•°æ®é›†ç®¡ç†
"""

import os
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional, Union

from ..core.llm import LLMClient
from ..core.index import IndexManager
from ..utils.display import Display
from .query_engine import QueryEngine


class ManageHandler:
    """æ•°æ®ç®¡ç†å¤„ç†å™¨ - æ”¯æŒå®Œæ•´çš„æ™ºèƒ½æŸ¥è¯¢ç³»ç»Ÿ"""
    
    # è§£ææ„å›¾çš„ç³»ç»Ÿæç¤º - æ”¯æŒå®Œæ•´çš„æŸ¥è¯¢è¯­æ³•
    PARSE_INTENT_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡ä»¶ç®¡ç†åŠ©æ‰‹ã€‚è§£æç”¨æˆ·çš„æ–‡ä»¶ç®¡ç†å’ŒæŸ¥è¯¢æŒ‡ä»¤ã€‚

ã€é‡è¦ã€‘æ•°æ®é›†é»˜è®¤å­˜å‚¨åœ¨ data/datasets/ ç›®å½•ä¸‹ã€‚

è¯·è¿”å› JSON æ ¼å¼ï¼š
{
  "action": "move | delete | copy | list | stats | export",
  "source": "æºè·¯å¾„ï¼ˆmove/delete/copyï¼‰",
  "target": "ç›®æ ‡è·¯å¾„ï¼ˆmove/copy/exportï¼‰",
  "query": {  // list/stats æ“ä½œçš„æŸ¥è¯¢è§„æ ¼
    // === è¿‡æ»¤ ===
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],  // å…³é”®è¯è¿‡æ»¤ï¼ˆANDå…³ç³»ï¼‰
    "keywords_mode": "and",  // æˆ– "or"
    "conditions": [  // æ¡ä»¶è¿‡æ»¤
      {"field": "å­—æ®µå", "op": "æ“ä½œç¬¦", "value": å€¼}
    ],
    
    // === å¤šç»„æŸ¥è¯¢ï¼ˆORå…³ç³»ï¼‰ ===
    "or_groups": [
      {"keywords": [...], "conditions": [...]},
      {"keywords": [...], "conditions": [...]}
    ],
    
    // === æ’åº ===
    "sort": "å­—æ®µå",  // æˆ– [{"field": "...", "order": "asc/desc"}]
    "sort_order": "asc",  // æˆ– "desc"
    
    // === åˆ†é¡µ ===
    "limit": 10,  // è¿”å›æ•°é‡
    "offset": 0,  // è·³è¿‡æ•°é‡
    
    // === èšåˆç»Ÿè®¡ ===
    "aggregate": "count" æˆ– "sum:nodes" æˆ– "avg:edges" æˆ– "group:source"
  }
}

=== åŠ¨ä½œè¯´æ˜ ===
- list: åˆ—å‡ºæ•°æ®é›†ï¼ˆæ”¯æŒæ™ºèƒ½æŸ¥è¯¢ï¼‰
- stats: ç»Ÿè®¡åˆ†æï¼ˆèšåˆæ“ä½œï¼‰
- move: ç§»åŠ¨æ–‡ä»¶/ç›®å½•
- copy: å¤åˆ¶æ–‡ä»¶/ç›®å½•
- delete: åˆ é™¤æ–‡ä»¶/ç›®å½•
- export: å¯¼å‡ºæŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶

=== å­—æ®µåï¼ˆæ”¯æŒåˆ«åï¼‰ ===
- èŠ‚ç‚¹æ•°: nodes, n, node, vertices, v
- è¾¹æ•°: edges, e, m, edge, links
- æ–‡ä»¶å¤§å°: size, filesize
- åç§°: name
- æè¿°: description, desc
- æ¥æº: source_url, source, url
- è·¯å¾„: local_path, path
- æ—¶é—´: crawl_time, time, date
- æ ‡ç­¾: tags, tag
- æ ¼å¼: format, type

=== æ“ä½œç¬¦ ===
æ•°å€¼æ¯”è¾ƒ: >, >=, <, <=, ==, !=
å­—ç¬¦ä¸²: contains, not_contains, startswith, endswith, regex
èŒƒå›´: betweenï¼ˆå€¼ä¸º [min, max]ï¼‰
åˆ—è¡¨: in, not_inï¼ˆå€¼ä¸ºæ•°ç»„ï¼‰
ç©ºå€¼: is_null, is_not_null

=== æ’åº ===
- å•å­—æ®µ: "sort": "nodes", "sort_order": "desc"
- å¤šå­—æ®µ: "sort": [{"field": "source", "order": "asc"}, {"field": "nodes", "order": "desc"}]
- ç®€å†™: "sort": ["-nodes", "name"]ï¼ˆ-è¡¨ç¤ºé™åºï¼‰

=== èšåˆç»Ÿè®¡ ===
- count: è®¡æ•°
- sum:å­—æ®µ: æ±‚å’Œï¼ˆå¦‚ sum:nodesï¼‰
- avg:å­—æ®µ: å¹³å‡å€¼
- min:å­—æ®µ, max:å­—æ®µ: æœ€å°/æœ€å¤§å€¼
- group:å­—æ®µ: åˆ†ç»„ç»Ÿè®¡
- distinct:å­—æ®µ: å»é‡è®¡æ•°

=== å…³é”®è¯ç¿»è¯‘ï¼ˆä¸­æ–‡â†’è‹±æ–‡ï¼‰ ===
è·¯ç½‘/é“è·¯ç½‘ç»œ -> road
ç¤¾äº¤ç½‘ç»œ -> social
å¼•ç”¨ç½‘ç»œ -> citation
é€šä¿¡ç½‘ç»œ -> communication
ç”Ÿç‰©ç½‘ç»œ -> bio

=== ç¤ºä¾‹ ===

ã€åŸºæœ¬åˆ—è¡¨ã€‘
"åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†" -> {"action": "list"}
"åˆ—å‡º konect æ•°æ®é›†" -> {"action": "list", "query": {"keywords": ["konect"]}}

ã€æ¡ä»¶è¿‡æ»¤ã€‘
"åˆ—å‡ºèŠ‚ç‚¹æ•°å¤§äº1000çš„æ•°æ®é›†" -> {"action": "list", "query": {"conditions": [{"field": "nodes", "op": ">", "value": 1000}]}}
"æ‰¾å‡º 100åˆ°10000 ä¸ªèŠ‚ç‚¹çš„æ•°æ®" -> {"action": "list", "query": {"conditions": [{"field": "nodes", "op": "between", "value": [100, 10000]}]}}

ã€å¤šå…³é”®è¯ã€‘
"snap çš„è·¯ç½‘æ•°æ®" -> {"action": "list", "query": {"keywords": ["snap", "road"]}}

ã€å¤åˆæ¡ä»¶ã€‘
"snap è·¯ç½‘é‡Œ n>1000 çš„" -> {"action": "list", "query": {"keywords": ["snap", "road"], "conditions": [{"field": "nodes", "op": ">", "value": 1000}]}}

ã€å¤šç»„æŸ¥è¯¢ï¼ˆORï¼‰ã€‘
"snapç¤¾äº¤ç½‘ç»œn>1000 å’Œ konectè·¯ç½‘m<100000" -> {"action": "list", "query": {"or_groups": [
  {"keywords": ["snap", "social"], "conditions": [{"field": "nodes", "op": ">", "value": 1000}]},
  {"keywords": ["konect", "road"], "conditions": [{"field": "edges", "op": "<", "value": 100000}]}
]}}

ã€æ’åºã€‘
"æŒ‰èŠ‚ç‚¹æ•°ä»å¤§åˆ°å°æ’åˆ—" -> {"action": "list", "query": {"sort": "nodes", "sort_order": "desc"}}
"å‰10ä¸ªæœ€å¤§çš„æ•°æ®é›†" -> {"action": "list", "query": {"sort": "nodes", "sort_order": "desc", "limit": 10}}

ã€ç»Ÿè®¡ã€‘
"ç»Ÿè®¡æœ‰å¤šå°‘æ•°æ®é›†" -> {"action": "stats", "query": {"aggregate": "count"}}
"æ€»èŠ‚ç‚¹æ•°æ˜¯å¤šå°‘" -> {"action": "stats", "query": {"aggregate": "sum:nodes"}}
"æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡" -> {"action": "stats", "query": {"aggregate": "group:source"}}
"snapæ•°æ®çš„å¹³å‡èŠ‚ç‚¹æ•°" -> {"action": "stats", "query": {"keywords": ["snap"], "aggregate": "avg:nodes"}}

ã€å¯¼å‡ºã€‘
"æŠŠ snap æ•°æ®é›†å¯¼å‡ºåˆ° result.json" -> {"action": "export", "query": {"keywords": ["snap"]}, "target": "result.json"}

ã€æ–‡ä»¶æ“ä½œã€‘
"æŠŠ snap ç§»åˆ° backup" -> {"action": "move", "source": "data/datasets/snap.stanford.edu", "target": "data/datasets/backup"}
"åˆ é™¤ facebook æ•°æ®é›†" -> {"action": "delete", "source": "data/datasets/snap.stanford.edu/facebook"}"""

    def __init__(self):
        self.llm = LLMClient()
        self.index = IndexManager()
        self.display = Display()
        self.query_engine = QueryEngine()
    
    def handle(self, prompt: str) -> dict:
        """
        å¤„ç†ç®¡ç†è¯·æ±‚
        
        Args:
            prompt: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            
        Returns:
            æ“ä½œç»“æœ
        """
        # 1. è§£ææ„å›¾
        self.display.print_status("æ­£åœ¨è§£ææŒ‡ä»¤...")
        intent = self._parse_intent(prompt)
        
        action = intent.get("action", "").lower()
        source = intent.get("source", "")
        target = intent.get("target", "")
        query_spec = intent.get("query", {})
        
        # å…¼å®¹æ—§æ ¼å¼ï¼šconditions å’Œ queries
        if not query_spec:
            conditions = intent.get("conditions", [])
            queries = intent.get("queries", [])
            source_kw = intent.get("source", "")
            
            if queries:
                query_spec = {"or_groups": queries}
            elif (isinstance(source_kw, (str, list)) and source_kw) or conditions:
                if isinstance(source_kw, str) and source_kw and action == "list":
                    query_spec["keywords"] = [source_kw]
                elif isinstance(source_kw, list):
                    query_spec["keywords"] = source_kw
                if conditions:
                    query_spec["conditions"] = conditions
        
        self.display.print_status(f"åŠ¨ä½œ: {action}")
        if source and action in ["move", "copy", "delete"]:
            self.display.print_status(f"æº: {source}")
        if target:
            self.display.print_status(f"ç›®æ ‡: {target}")
        if query_spec:
            self._print_query_spec(query_spec)
        
        # 2. æ‰§è¡Œå¯¹åº”æ“ä½œ
        if action == "list":
            return self._handle_list(query_spec)
        elif action == "stats":
            return self._handle_stats(query_spec)
        elif action == "export":
            return self._handle_export(query_spec, target)
        elif action == "move":
            return self._handle_move(source, target)
        elif action == "copy":
            return self._handle_copy(source, target)
        elif action == "delete":
            return self._handle_delete(source)
        else:
            self.display.print_error(f"æœªçŸ¥æ“ä½œ: {action}")
            return {"success": False, "error": f"æœªçŸ¥æ“ä½œ: {action}"}
    
    def _print_query_spec(self, spec: dict):
        """æ‰“å°æŸ¥è¯¢è§„æ ¼æ‘˜è¦"""
        parts = []
        if spec.get("keywords"):
            parts.append(f"å…³é”®è¯: {spec['keywords']}")
        if spec.get("conditions"):
            # æ ¼å¼åŒ–æ¡ä»¶ä¸ºæ˜“è¯»å½¢å¼
            cond_strs = []
            for c in spec['conditions']:
                field = c.get('field', '?')
                op = c.get('op', '?')
                value = c.get('value', '?')
                cond_strs.append(f"{field}{op}{value}")
            parts.append(f"æ¡ä»¶: {', '.join(cond_strs)}")
        if spec.get("or_groups"):
            # æ ¼å¼åŒ–å¤šç»„æŸ¥è¯¢
            group_strs = []
            for i, g in enumerate(spec['or_groups'], 1):
                kws = g.get('keywords', [])
                conds = g.get('conditions', [])
                g_parts = []
                if kws:
                    g_parts.append(f"å…³é”®è¯={kws}")
                if conds:
                    c_strs = [f"{c.get('field')}{c.get('op')}{c.get('value')}" for c in conds]
                    g_parts.append(f"æ¡ä»¶={c_strs}")
                group_strs.append(f"({' '.join(g_parts)})")
            parts.append(f"å¤šç»„æŸ¥è¯¢: {' OR '.join(group_strs)}")
        if spec.get("sort"):
            order = spec.get('sort_order', 'asc')
            parts.append(f"æ’åº: {spec['sort']} {order}")
        if spec.get("limit"):
            parts.append(f"é™åˆ¶: {spec['limit']}")
        if spec.get("aggregate"):
            parts.append(f"èšåˆ: {spec['aggregate']}")
        if parts:
            self.display.print_status(f"æŸ¥è¯¢: {', '.join(parts)}")
    
    def _parse_intent(self, prompt: str) -> dict:
        """è§£æç”¨æˆ·æ„å›¾"""
        try:
            # è·å–å½“å‰æ•°æ®ç›®å½•ç»“æ„ï¼Œå¸®åŠ©LLMç†è§£ä¸Šä¸‹æ–‡
            dir_context = self._get_directory_context()
            
            user_message = f"""å½“å‰æ•°æ®ç›®å½•ç»“æ„ï¼š
{dir_context}

ç”¨æˆ·æŒ‡ä»¤: {prompt}"""
            
            result = self.llm.chat_json(
                self.PARSE_INTENT_SYSTEM,
                user_message
            )
            
            if "action" not in result:
                raise ValueError("æœªèƒ½è§£æå‡ºæ“ä½œç±»å‹")
            
            return result
            
        except Exception as e:
            raise ValueError(f"è§£ææ„å›¾å¤±è´¥: {e}")
    
    def _get_directory_context(self, max_depth: int = 3) -> str:
        """
        è·å– data/datasets ç›®å½•ç»“æ„ä½œä¸ºä¸Šä¸‹æ–‡
        
        Args:
            max_depth: æœ€å¤§éå†æ·±åº¦
            
        Returns:
            ç›®å½•ç»“æ„çš„æ–‡æœ¬è¡¨ç¤º
        """
        base_dir = Path(__file__).parent.parent.parent / "data" / "datasets"
        
        if not base_dir.exists():
            return "data/datasets/ (ç›®å½•ä¸å­˜åœ¨)"
        
        lines = ["data/datasets/"]
        
        def _scan_dir(path: Path, prefix: str, depth: int):
            if depth > max_depth:
                return
            
            try:
                items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))
            except PermissionError:
                return
            
            # é™åˆ¶æ¯å±‚æ˜¾ç¤ºçš„é¡¹ç›®æ•°
            dirs = [item for item in items if item.is_dir()]
            files = [item for item in items if item.is_file()]
            
            # æ˜¾ç¤ºæ‰€æœ‰ç›®å½•
            for i, item in enumerate(dirs):
                is_last_dir = (i == len(dirs) - 1) and not files
                connector = "â””â”€â”€ " if is_last_dir else "â”œâ”€â”€ "
                lines.append(f"{prefix}{connector}{item.name}/")
                
                # é€’å½’å­ç›®å½•
                new_prefix = prefix + ("    " if is_last_dir else "â”‚   ")
                _scan_dir(item, new_prefix, depth + 1)
            
            # æ˜¾ç¤ºæ–‡ä»¶ï¼ˆæœ€å¤š5ä¸ªï¼Œè¶…å‡ºæ˜¾ç¤ºæ•°é‡ï¼‰
            if files:
                shown_files = files[:5]
                for i, item in enumerate(shown_files):
                    is_last = (i == len(shown_files) - 1) and (len(files) <= 5)
                    connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
                    try:
                        size = item.stat().st_size
                        size_str = self._format_size(size)
                        lines.append(f"{prefix}{connector}{item.name} ({size_str})")
                    except:
                        lines.append(f"{prefix}{connector}{item.name}")
                
                if len(files) > 5:
                    lines.append(f"{prefix}â””â”€â”€ ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
        
        _scan_dir(base_dir, "", 1)
        
        return "\n".join(lines) if len(lines) > 1 else "data/datasets/ (ç©ºç›®å½•)"
    
    def _format_size(self, size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size < 1024:
            return f"{size}B"
        elif size < 1024 * 1024:
            return f"{size/1024:.1f}KB"
        elif size < 1024 * 1024 * 1024:
            return f"{size/1024/1024:.1f}MB"
        else:
            return f"{size/1024/1024/1024:.1f}GB"
    
    def _resolve_source(self, source: str) -> Optional[Path]:
        """
        è§£ææºè·¯å¾„
        
        Args:
            source: è·¯å¾„æˆ–æ•°æ®é›†æè¿°
            
        Returns:
            è§£æåçš„è·¯å¾„
        """
        # å…ˆå°è¯•ä½œä¸ºè·¯å¾„
        path = Path(source)
        if path.exists():
            return path.resolve()
        
        # å°è¯•è¡¥å…¨ data/datasets å‰ç¼€
        base_dir = Path(__file__).parent.parent.parent / "data" / "datasets"
        prefixed_path = base_dir / source.lstrip('/')
        if prefixed_path.exists():
            return prefixed_path.resolve()
        
        # å°è¯•åœ¨ç´¢å¼•ä¸­æŸ¥æ‰¾
        datasets = self.index.find_by_name(source)
        
        if not datasets:
            # å°è¯•å…³é”®è¯æœç´¢
            keywords = source.split()
            datasets = self.index.search(keywords)
        
        if len(datasets) == 1:
            local_path = datasets[0].get("local_path")
            if local_path and Path(local_path).exists():
                return Path(local_path).resolve()
        elif len(datasets) > 1:
            # è®©ç”¨æˆ·é€‰æ‹©
            self.display.print_warning(f"æ‰¾åˆ°å¤šä¸ªåŒ¹é…çš„æ•°æ®é›†:")
            for i, ds in enumerate(datasets, 1):
                print(f"  {i}. {ds.get('name')} - {ds.get('local_path')}")
            
            choice = input("è¯·è¾“å…¥åºå·é€‰æ‹©: ").strip()
            try:
                idx = int(choice) - 1
                if 0 <= idx < len(datasets):
                    local_path = datasets[idx].get("local_path")
                    if local_path and Path(local_path).exists():
                        return Path(local_path).resolve()
            except ValueError:
                pass
        
        return None
    
    def _handle_list(self, query_spec: dict = None) -> dict:
        """
        åˆ—å‡ºæ•°æ®é›†ï¼ˆä½¿ç”¨æ™ºèƒ½æŸ¥è¯¢å¼•æ“ï¼‰
        
        Args:
            query_spec: æŸ¥è¯¢è§„æ ¼å­—å…¸
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        all_datasets = self.index.get_all()
        query_spec = query_spec or {}
        
        # ä½¿ç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢
        result = self.query_engine.query(all_datasets, query_spec)
        datasets = result.get("data", [])
        
        if not datasets:
            if query_spec:
                self.display.print_warning("æœªæ‰¾åˆ°åŒ¹é…æ¡ä»¶çš„æ•°æ®é›†")
            else:
                self.display.print_warning("ç´¢å¼•ä¸­æ²¡æœ‰æ•°æ®é›†")
        
        self.display.print_datasets(datasets)
        
        # å¦‚æœæœ‰èšåˆç»“æœï¼Œä¹Ÿæ˜¾ç¤º
        if result.get("aggregation"):
            self._print_aggregation(result["aggregation"])
        
        return {
            "success": True,
            "action": "list",
            "query": query_spec,
            "count": result["count"],
            "total": result["total"]
        }
    
    def _handle_stats(self, query_spec: dict = None) -> dict:
        """
        ç»Ÿè®¡åˆ†æ
        
        Args:
            query_spec: æŸ¥è¯¢è§„æ ¼ï¼ˆå¿…é¡»åŒ…å« aggregateï¼‰
            
        Returns:
            ç»Ÿè®¡ç»“æœ
        """
        all_datasets = self.index.get_all()
        query_spec = query_spec or {}
        
        # ç¡®ä¿æœ‰èšåˆæ“ä½œ
        if not query_spec.get("aggregate"):
            query_spec["aggregate"] = "count"
        
        # ä½¿ç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢
        result = self.query_engine.query(all_datasets, query_spec)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        self._print_aggregation(result.get("aggregation", {}))
        
        # å¦‚æœç­›é€‰åæœ‰æ•°æ®ï¼Œä¹Ÿæ˜¾ç¤ºæ•°é‡
        filtered_count = result.get("count", 0)
        total = result.get("total", 0)
        if filtered_count < total:
            self.display.print_status(f"ï¼ˆç­›é€‰å {filtered_count} / æ€»è®¡ {total} ä¸ªæ•°æ®é›†ï¼‰")
        
        return {
            "success": True,
            "action": "stats",
            "query": query_spec,
            "aggregation": result.get("aggregation"),
            "count": filtered_count,
            "total": total
        }
    
    def _handle_export(self, query_spec: dict, target: str) -> dict:
        """
        å¯¼å‡ºæŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            query_spec: æŸ¥è¯¢è§„æ ¼
            target: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        import json
        
        all_datasets = self.index.get_all()
        query_spec = query_spec or {}
        
        # ä½¿ç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢
        result = self.query_engine.query(all_datasets, query_spec)
        datasets = result.get("data", [])
        
        if not datasets:
            self.display.print_warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return {"success": False, "error": "æ²¡æœ‰æ•°æ®å¯å¯¼å‡º"}
        
        # è§£æç›®æ ‡è·¯å¾„
        target_path = Path(target)
        if not target_path.is_absolute():
            base_dir = Path(__file__).parent.parent.parent
            target_path = base_dir / target
        target_path = target_path.resolve()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        target_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®æ‰©å±•åå†³å®šæ ¼å¼
        ext = target_path.suffix.lower()
        
        try:
            if ext == ".json":
                with open(target_path, "w", encoding="utf-8") as f:
                    json.dump(datasets, f, ensure_ascii=False, indent=2)
            elif ext == ".csv":
                self._export_csv(datasets, target_path)
            else:
                # é»˜è®¤ JSON
                with open(target_path, "w", encoding="utf-8") as f:
                    json.dump(datasets, f, ensure_ascii=False, indent=2)
            
            self.display.print_success(f"å·²å¯¼å‡º {len(datasets)} æ¡è®°å½•åˆ°: {target_path}")
            
            return {
                "success": True,
                "action": "export",
                "target": str(target_path),
                "count": len(datasets)
            }
            
        except Exception as e:
            self.display.print_error(f"å¯¼å‡ºå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _export_csv(self, datasets: list, path: Path):
        """å¯¼å‡ºä¸º CSV æ ¼å¼"""
        import csv
        
        if not datasets:
            return
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µ
        fields = set()
        for ds in datasets:
            fields.update(ds.keys())
            if "properties" in ds:
                fields.update(f"prop_{k}" for k in ds["properties"].keys())
        fields = sorted(fields)
        
        with open(path, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fields)
            writer.writeheader()
            
            for ds in datasets:
                row = dict(ds)
                # å±•å¼€ properties
                if "properties" in ds:
                    for k, v in ds["properties"].items():
                        row[f"prop_{k}"] = v
                writer.writerow(row)
    
    def _print_aggregation(self, agg: dict):
        """æ‰“å°èšåˆç»“æœ"""
        if not agg:
            return
        
        print("\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        for key, value in agg.items():
            if isinstance(value, dict):
                # åˆ†ç»„ç»Ÿè®¡
                print(f"  {key}:")
                for k, v in sorted(value.items(), key=lambda x: -x[1] if isinstance(x[1], (int, float)) else x[0]):
                    print(f"    {k}: {v}")
            elif isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        print()
    
    def _handle_move(self, source: str, target: str) -> dict:
        """ç§»åŠ¨æ•°æ®"""
        # è§£ææºè·¯å¾„
        source_path = self._resolve_source(source)
        if not source_path:
            self.display.print_error(f"æ‰¾ä¸åˆ°æºè·¯å¾„: {source}")
            return {"success": False, "error": f"æ‰¾ä¸åˆ°: {source}"}
        
        # è§£æç›®æ ‡è·¯å¾„ï¼ˆä¹Ÿæ”¯æŒè‡ªåŠ¨è¡¥å…¨ data/datasets å‰ç¼€ï¼‰
        target_path = Path(target)
        if not target_path.is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…¨ data/datasets
            base_dir = Path(__file__).parent.parent.parent / "data" / "datasets"
            if target.startswith("data/datasets/") or target.startswith("data\\datasets\\"):
                target_path = Path(__file__).parent.parent.parent / target
            else:
                target_path = base_dir / target.lstrip('/')
        target_path = target_path.resolve()
        
        # æ£€æŸ¥æ˜¯å¦å°è¯•å°†ç›®å½•ç§»åŠ¨åˆ°è‡ªèº«å†…éƒ¨ï¼ˆä¸‹æ²‰æ“ä½œï¼‰
        is_sink_operation = False
        try:
            # æ£€æŸ¥ target_path æ˜¯å¦åœ¨ source_path å†…éƒ¨
            target_path.relative_to(source_path)
            is_sink_operation = True
        except ValueError:
            # ä¸åœ¨å†…éƒ¨ï¼Œæ­£å¸¸ç§»åŠ¨
            pass
        
        if is_sink_operation:
            # è¿™æ˜¯"ä¸‹æ²‰"æ“ä½œï¼šæŠŠç›®å½•å†…å®¹ç§»åŠ¨åˆ°å…¶å­ç›®å½•
            return self._handle_sink_move(source_path, target_path)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯"ä¸Šæµ®"æ“ä½œï¼ˆæŠŠå­ç›®å½•å†…å®¹ç§»åŠ¨åˆ°çˆ¶ç›®å½•ï¼‰
        is_float_operation = False
        try:
            # æ£€æŸ¥ source_path æ˜¯å¦åœ¨ target_path å†…éƒ¨
            source_path.relative_to(target_path)
            is_float_operation = True
        except ValueError:
            pass
        
        if is_float_operation:
            # è¿™æ˜¯"ä¸Šæµ®"æ“ä½œï¼šæŠŠå­ç›®å½•å†…å®¹ç§»åŠ¨åˆ°çˆ¶ç›®å½•
            return self._handle_float_move(source_path, target_path)
        
        # æ˜¾ç¤ºé¢„è§ˆ
        self.display.print_status(f"å³å°†æ‰§è¡Œç§»åŠ¨æ“ä½œ:")
        print(f"  ä»: {source_path}")
        print(f"  åˆ°: {target_path}")
        
        # ç¡®è®¤
        if not self.display.confirm("ç¡®è®¤æ‰§è¡Œæ­¤æ“ä½œï¼Ÿ"):
            self.display.print_warning("æ“ä½œå·²å–æ¶ˆ")
            return {"success": False, "cancelled": True}
        
        try:
            # ç¡®ä¿ç›®æ ‡çˆ¶ç›®å½•å­˜åœ¨
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ç§»åŠ¨
            shutil.move(str(source_path), str(target_path))
            
            # æ›´æ–°ç´¢å¼•
            updated_count = self.index.update_path(str(source_path), str(target_path))
            
            self.display.print_success(f"å·²ç§»åŠ¨åˆ°: {target_path}")
            if updated_count > 0:
                self.display.print_status(f"å·²æ›´æ–° {updated_count} æ¡ç´¢å¼•è®°å½•")
            
            return {
                "success": True,
                "action": "move",
                "source": str(source_path),
                "target": str(target_path)
            }
            
        except Exception as e:
            self.display.print_error(f"ç§»åŠ¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _handle_sink_move(self, source_path: Path, target_path: Path) -> dict:
        """
        å¤„ç†"ä¸‹æ²‰"ç§»åŠ¨ï¼šå°†ç›®å½•å†…å®¹ç§»åŠ¨åˆ°å…¶å­ç›®å½•ä¸­
        
        ä¾‹å¦‚ï¼šæŠŠ snap.stanford.edu/ ä¸‹çš„å†…å®¹ç§»åŠ¨åˆ° snap.stanford.edu/social/
        """
        self.display.print_status(f"æ£€æµ‹åˆ°ä¸‹æ²‰æ“ä½œï¼šå°†ç›®å½•å†…å®¹ç§»åŠ¨åˆ°å­ç›®å½•")
        
        # è·å–æºç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®ï¼ˆæ’é™¤ç›®æ ‡å­ç›®å½•ï¼‰
        items_to_move = []
        target_relative = target_path.relative_to(source_path)
        target_first_part = target_relative.parts[0] if target_relative.parts else None
        
        for item in source_path.iterdir():
            # è·³è¿‡ç›®æ ‡å­ç›®å½•çš„ç¬¬ä¸€çº§ç›®å½•
            if item.name == target_first_part:
                continue
            items_to_move.append(item)
        
        if not items_to_move:
            self.display.print_warning("æºç›®å½•ä¸‹æ²¡æœ‰éœ€è¦ç§»åŠ¨çš„å†…å®¹")
            return {"success": False, "error": "æ²¡æœ‰å†…å®¹éœ€è¦ç§»åŠ¨"}
        
        # æ˜¾ç¤ºé¢„è§ˆ
        self.display.print_status(f"å³å°†æ‰§è¡Œä¸‹æ²‰ç§»åŠ¨æ“ä½œ:")
        print(f"  æºç›®å½•: {source_path}")
        print(f"  ç›®æ ‡ç›®å½•: {target_path}")
        print(f"  å°†ç§»åŠ¨ä»¥ä¸‹ {len(items_to_move)} ä¸ªé¡¹ç›®:")
        for item in items_to_move[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            item_type = "ğŸ“" if item.is_dir() else "ğŸ“„"
            print(f"    {item_type} {item.name}")
        if len(items_to_move) > 10:
            print(f"    ... è¿˜æœ‰ {len(items_to_move) - 10} ä¸ªé¡¹ç›®")
        
        # ç¡®è®¤
        if not self.display.confirm("ç¡®è®¤æ‰§è¡Œæ­¤æ“ä½œï¼Ÿ"):
            self.display.print_warning("æ“ä½œå·²å–æ¶ˆ")
            return {"success": False, "cancelled": True}
        
        try:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            target_path.mkdir(parents=True, exist_ok=True)
            
            moved_count = 0
            updated_index_count = 0
            for item in items_to_move:
                dest = target_path / item.name
                shutil.move(str(item), str(dest))
                moved_count += 1
                
                # æ›´æ–°ç´¢å¼•ä¸­çš„è·¯å¾„
                updated_index_count += self.index.update_path(str(item), str(dest))
            
            self.display.print_success(f"å·²å°† {moved_count} ä¸ªé¡¹ç›®ç§»åŠ¨åˆ°: {target_path}")
            if updated_index_count > 0:
                self.display.print_status(f"å·²æ›´æ–° {updated_index_count} æ¡ç´¢å¼•è®°å½•")
            
            return {
                "success": True,
                "action": "sink_move",
                "source": str(source_path),
                "target": str(target_path),
                "moved_count": moved_count
            }
            
        except Exception as e:
            self.display.print_error(f"ç§»åŠ¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _handle_float_move(self, source_path: Path, target_path: Path) -> dict:
        """
        å¤„ç†"ä¸Šæµ®"ç§»åŠ¨ï¼šå°†å­ç›®å½•å†…å®¹ç§»åŠ¨åˆ°çˆ¶ç›®å½•ä¸­
        
        ä¾‹å¦‚ï¼šæŠŠ snap.stanford.edu/social/ ä¸‹çš„å†…å®¹ç§»åŠ¨åˆ° snap.stanford.edu/
        """
        self.display.print_status(f"æ£€æµ‹åˆ°ä¸Šæµ®æ“ä½œï¼šå°†å­ç›®å½•å†…å®¹ç§»åŠ¨åˆ°çˆ¶ç›®å½•")
        
        # è·å–æºç›®å½•ä¸‹çš„æ‰€æœ‰é¡¹ç›®
        items_to_move = list(source_path.iterdir())
        
        if not items_to_move:
            self.display.print_warning("æºç›®å½•ä¸‹æ²¡æœ‰éœ€è¦ç§»åŠ¨çš„å†…å®¹")
            return {"success": False, "error": "æ²¡æœ‰å†…å®¹éœ€è¦ç§»åŠ¨"}
        
        # æ£€æŸ¥å†²çªï¼šç›®æ ‡ç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨åŒåé¡¹ç›®
        conflicts = []
        for item in items_to_move:
            dest = target_path / item.name
            if dest.exists() and dest != source_path:
                conflicts.append(item.name)
        
        # æ˜¾ç¤ºé¢„è§ˆ
        self.display.print_status(f"å³å°†æ‰§è¡Œä¸Šæµ®ç§»åŠ¨æ“ä½œ:")
        print(f"  æºç›®å½•: {source_path}")
        print(f"  ç›®æ ‡ç›®å½•: {target_path}")
        print(f"  å°†ç§»åŠ¨ä»¥ä¸‹ {len(items_to_move)} ä¸ªé¡¹ç›®:")
        for item in items_to_move[:10]:
            item_type = "ğŸ“" if item.is_dir() else "ğŸ“„"
            print(f"    {item_type} {item.name}")
        if len(items_to_move) > 10:
            print(f"    ... è¿˜æœ‰ {len(items_to_move) - 10} ä¸ªé¡¹ç›®")
        
        if conflicts:
            self.display.print_warning(f"âš ï¸ ä»¥ä¸‹ {len(conflicts)} ä¸ªé¡¹ç›®åœ¨ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–:")
            for name in conflicts[:5]:
                print(f"    - {name}")
            if len(conflicts) > 5:
                print(f"    ... è¿˜æœ‰ {len(conflicts) - 5} ä¸ª")
        
        # ç¡®è®¤
        if not self.display.confirm("ç¡®è®¤æ‰§è¡Œæ­¤æ“ä½œï¼Ÿ"):
            self.display.print_warning("æ“ä½œå·²å–æ¶ˆ")
            return {"success": False, "cancelled": True}
        
        try:
            moved_count = 0
            updated_index_count = 0
            
            for item in items_to_move:
                dest = target_path / item.name
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ä¸”ä¸æ˜¯æºç›®å½•æœ¬èº«ï¼Œéœ€è¦å…ˆåˆ é™¤
                if dest.exists() and dest != source_path:
                    if dest.is_dir():
                        shutil.rmtree(str(dest))
                    else:
                        dest.unlink()
                
                shutil.move(str(item), str(dest))
                moved_count += 1
                
                # æ›´æ–°ç´¢å¼•ä¸­çš„è·¯å¾„
                updated_index_count += self.index.update_path(str(item), str(dest))
            
            # ç§»åŠ¨å®Œæˆåï¼Œåˆ é™¤ç©ºçš„æºç›®å½•
            if source_path.exists() and not any(source_path.iterdir()):
                source_path.rmdir()
                self.display.print_status(f"å·²åˆ é™¤ç©ºç›®å½•: {source_path.name}")
            
            self.display.print_success(f"å·²å°† {moved_count} ä¸ªé¡¹ç›®ç§»åŠ¨åˆ°: {target_path}")
            if updated_index_count > 0:
                self.display.print_status(f"å·²æ›´æ–° {updated_index_count} æ¡ç´¢å¼•è®°å½•")
            
            return {
                "success": True,
                "action": "float_move",
                "source": str(source_path),
                "target": str(target_path),
                "moved_count": moved_count
            }
            
        except Exception as e:
            self.display.print_error(f"ç§»åŠ¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _handle_copy(self, source: str, target: str) -> dict:
        """å¤åˆ¶æ•°æ®"""
        # è§£ææºè·¯å¾„
        source_path = self._resolve_source(source)
        if not source_path:
            self.display.print_error(f"æ‰¾ä¸åˆ°æºè·¯å¾„: {source}")
            return {"success": False, "error": f"æ‰¾ä¸åˆ°: {source}"}
        
        # è§£æç›®æ ‡è·¯å¾„ï¼ˆä¹Ÿæ”¯æŒè‡ªåŠ¨è¡¥å…¨ data/datasets å‰ç¼€ï¼‰
        target_path = Path(target)
        if not target_path.is_absolute():
            base_dir = Path(__file__).parent.parent.parent / "data" / "datasets"
            if target.startswith("data/datasets/") or target.startswith("data\\datasets\\"):
                target_path = Path(__file__).parent.parent.parent / target
            else:
                target_path = base_dir / target.lstrip('/')
        target_path = target_path.resolve()
        
        # æ£€æŸ¥æ˜¯å¦å°è¯•å°†ç›®å½•å¤åˆ¶åˆ°è‡ªèº«å†…éƒ¨
        try:
            target_path.relative_to(source_path)
            self.display.print_error(f"æ— æ³•å°†ç›®å½•å¤åˆ¶åˆ°å…¶è‡ªèº«å†…éƒ¨")
            return {"success": False, "error": "æ— æ³•å°†ç›®å½•å¤åˆ¶åˆ°å…¶è‡ªèº«å†…éƒ¨"}
        except ValueError:
            pass
        
        # æ˜¾ç¤ºé¢„è§ˆ
        self.display.print_status(f"å³å°†æ‰§è¡Œå¤åˆ¶æ“ä½œ:")
        print(f"  ä»: {source_path}")
        print(f"  åˆ°: {target_path}")
        
        # ç¡®è®¤
        if not self.display.confirm("ç¡®è®¤æ‰§è¡Œæ­¤æ“ä½œï¼Ÿ"):
            self.display.print_warning("æ“ä½œå·²å–æ¶ˆ")
            return {"success": False, "cancelled": True}
        
        try:
            # ç¡®ä¿ç›®æ ‡çˆ¶ç›®å½•å­˜åœ¨
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶
            if source_path.is_dir():
                shutil.copytree(str(source_path), str(target_path))
            else:
                shutil.copy2(str(source_path), str(target_path))
            
            self.display.print_success(f"å·²å¤åˆ¶åˆ°: {target_path}")
            
            return {
                "success": True,
                "action": "copy",
                "source": str(source_path),
                "target": str(target_path)
            }
            
        except Exception as e:
            self.display.print_error(f"å¤åˆ¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _handle_delete(self, source: str) -> dict:
        """åˆ é™¤æ•°æ®"""
        # è§£ææºè·¯å¾„
        source_path = self._resolve_source(source)
        if not source_path:
            self.display.print_error(f"æ‰¾ä¸åˆ°æºè·¯å¾„: {source}")
            return {"success": False, "error": f"æ‰¾ä¸åˆ°: {source}"}
        
        # æ˜¾ç¤ºé¢„è§ˆ
        self.display.print_warning(f"å³å°†åˆ é™¤:")
        print(f"  è·¯å¾„: {source_path}")
        
        if source_path.is_dir():
            # ç»Ÿè®¡ç›®å½•å¤§å°
            total_size = sum(f.stat().st_size for f in source_path.rglob('*') if f.is_file())
            file_count = sum(1 for f in source_path.rglob('*') if f.is_file())
            print(f"  æ–‡ä»¶æ•°: {file_count}")
            print(f"  æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        
        # ç¡®è®¤ï¼ˆåˆ é™¤æ“ä½œéœ€è¦äºŒæ¬¡ç¡®è®¤ï¼‰
        if not self.display.confirm("âš ï¸ æ­¤æ“ä½œä¸å¯æ¢å¤ï¼ç¡®è®¤åˆ é™¤ï¼Ÿ"):
            self.display.print_warning("æ“ä½œå·²å–æ¶ˆ")
            return {"success": False, "cancelled": True}
        
        try:
            # ä»ç´¢å¼•ä¸­æŸ¥æ‰¾å¹¶åˆ é™¤è®°å½•
            dataset = self.index.find_by_path(str(source_path))
            if dataset:
                self.index.delete(dataset.get("id"))
            
            # åˆ é™¤æ–‡ä»¶
            if source_path.is_dir():
                shutil.rmtree(str(source_path))
            else:
                source_path.unlink()
            
            self.display.print_success(f"å·²åˆ é™¤: {source_path}")
            
            return {
                "success": True,
                "action": "delete",
                "deleted": str(source_path)
            }
            
        except Exception as e:
            self.display.print_error(f"åˆ é™¤å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
